{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-class Classification\n",
    "\n",
    "We want to assess the performance of a NN trained with a privacy engine based on SGD algorithm.\n",
    "The dataset is public and references to its license could be found in the README.md in /data subdirectory of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call it the Ordinary stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "# These are the new cowboys in the town\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.pytorch import DataLoader\n",
    "from pyspark.sql import SparkSession\n",
    "import torch\n",
    "import opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entry point of our spark app\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"2-class classification\") \\\n",
    "        .getOrCreate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately there's no direct way to populate Spark's Dataframes from remote. So... here's a hack\n",
    "url = \"https://raw.githubusercontent.com/alessio-proietti/dp-sgd-notebook/main/data/bank-additional-full.csv\"\n",
    "df = pd.read_csv(url, delimiter=\";\")\n",
    "\n",
    "# This is the real thing, our dataframe exposed as Spark's Dataframe\n",
    "df = spark.createDataFrame(df)\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!PLEASE DO NOT EXECUTE THIS CELL!!!\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "# We index the string category with numbers\n",
    "stringIndexer = StringIndexer() \\\n",
    "    .setInputCols([\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]) \\\n",
    "    .setOutputCols([\"jobIndex\", \"maritalIndex\", \"educationIndex\", \"defaultIndex\", \"housingIndex\", \"loanIndex\", \"contactIndex\", \"monthIndex\", \"poutcomeIndex\"])\n",
    "\n",
    "stringModel = stringIndexer.fit(df)\n",
    "df = stringModel.transform(df).drop(\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\")\n",
    "\n",
    "# Note This is different from scikit-learnâ€™s OneHotEncoder, which keeps all categories. \n",
    "# The output vectors are sparse.\n",
    "ohe = OneHotEncoder() \\\n",
    "    .setInputCols([\"jobIndex\", \"maritalIndex\", \"educationIndex\", \"defaultIndex\", \"housingIndex\", \"loanIndex\", \"contactIndex\", \"monthIndex\", \"poutcomeIndex\"]) \\\n",
    "    .setOutputCols([\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"])\n",
    "\n",
    "oheModel = ohe.fit(df)\n",
    "df = oheModel.transform(df).drop(\"jobIndex\", \"maritalIndex\", \"educationIndex\", \"defaultIndex\", \"housingIndex\", \"loanIndex\", \"contactIndex\", \"monthIndex\", \"poutcomeIndex\")\n",
    "\n",
    "#to be deleted IS ONLY AN EXPERIMENT\n",
    "df = df.drop(\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"nr.employed\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only to be used in development. We want to run a toy model in PyTorch\n",
    "# !!! TO BE DELETED !!!\n",
    "df = df[['age', 'y']]\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of instances is quite large, we could attempt to have a tripartition of the dataset\n",
    "train, validation, test = df.randomSplit([3.0, 1.0, 1.0], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write on an Apache Parquet the results of the 'data wrangling'\n",
    "\n",
    "test \\\n",
    " .write \\\n",
    " .mode('overwrite') \\\n",
    " .parquet('data/spark_processed_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute with the actual path\n",
    "read = make_batch_reader('file:///path/to/dp-sgd-notebook/data/spark_processed_data')\n",
    "\n",
    "DataLoader(\n",
    "        read,\n",
    "        batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

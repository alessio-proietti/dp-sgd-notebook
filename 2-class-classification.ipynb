{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-class Classification\n",
    "\n",
    "The dataset is public and references to its license could be found in the README.md in /data subdirectory of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autodp\n",
    "#!pip install pyspark\n",
    "#!pip install mxnet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, Trainer\n",
    "from mxnet.gluon.data import DataLoader, ArrayDataset\n",
    "\n",
    "# shipped with the repo\n",
    "import dpdl_utils\n",
    "\n",
    "# import packages for DP\n",
    "from autodp import rdp_bank, rdp_acct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entry point of our spark app\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"2-class classification\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to download the dataset\n",
    "\n",
    "The dataset we use can downloaded from https://raw.githubusercontent.com/alessio-proietti/dp-sgd-notebook/main/data/bank-additional-full-new-label.csv. \n",
    "\n",
    "It's shipped with the repo itself though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- campaign: long (nullable = true)\n",
      " |-- pdays: long (nullable = true)\n",
      " |-- previous: long (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp_var_rate: double (nullable = true)\n",
      " |-- cons_price_idx: double (nullable = true)\n",
      " |-- con_conf_idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr_employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "-RECORD 0---------------------\n",
      " age            | 56          \n",
      " job            | housemaid   \n",
      " marital        | married     \n",
      " education      | basic.4y    \n",
      " default        | no          \n",
      " housing        | no          \n",
      " loan           | no          \n",
      " contact        | telephone   \n",
      " month          | may         \n",
      " day_of_week    | mon         \n",
      " duration       | 261         \n",
      " campaign       | 1           \n",
      " pdays          | 999         \n",
      " previous       | 0           \n",
      " poutcome       | nonexistent \n",
      " emp_var_rate   | 1.1         \n",
      " cons_price_idx | 93994.0     \n",
      " con_conf_idx   | -36.4       \n",
      " euribor3m      | 4857.0      \n",
      " nr_employed    | 5191.0      \n",
      " y              | no          \n",
      "-RECORD 1---------------------\n",
      " age            | 57          \n",
      " job            | services    \n",
      " marital        | married     \n",
      " education      | high.school \n",
      " default        | unknown     \n",
      " housing        | no          \n",
      " loan           | no          \n",
      " contact        | telephone   \n",
      " month          | may         \n",
      " day_of_week    | mon         \n",
      " duration       | 149         \n",
      " campaign       | 1           \n",
      " pdays          | 999         \n",
      " previous       | 0           \n",
      " poutcome       | nonexistent \n",
      " emp_var_rate   | 1.1         \n",
      " cons_price_idx | 93994.0     \n",
      " con_conf_idx   | -36.4       \n",
      " euribor3m      | 4857.0      \n",
      " nr_employed    | 5191.0      \n",
      " y              | no          \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pd.read_csv(\"data/bank-additional-full-new-label.csv\"))\n",
    "df.printSchema()\n",
    "df.show(n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericCols = [field for (field, dataType) in df.dtypes if ( dataType != \"string\" )]\n",
    "categoricalCols = [field for (field, dataType) in df.dtypes if (dataType == \"string\" and field != \"y\")]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "stringIndexerLabel = StringIndexer(inputCol=\"y\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "labelModel = stringIndexerLabel.fit(df)\n",
    "df = labelModel.transform(df)\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|            features|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|(53,[6,12,15,21,2...|       0.0|\n",
      "|  0.0|(53,[6,12,15,21,2...|       0.0|\n",
      "|  0.0|(53,[6,12,15,21,2...|       0.0|\n",
      "|  0.0|(53,[6,12,15,21,2...|       0.0|\n",
      "|  0.0|(53,[0,12,16,21,2...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = df.randomSplit([3.0, 1.0, 1.0], 24)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(train)\n",
    "predDF = pipelineModel.transform(train).select(\"label\",\"features\",\"prediction\")\n",
    "predDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6803682895002037"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "areaUnderROC = evaluator.evaluate(predDF)\n",
    "areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24748, 53) (24748,)\n"
     ]
    }
   ],
   "source": [
    "data_array = nd.array( [row.features.toArray() for row in predDF.collect()])\n",
    "label_array = nd.array( [row.label for row in predDF.collect()])\n",
    "print(data_array.shape, label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24748, 53])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = torch.tensor([row.features.toArray() for row in predDF.collect()])\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-class Classification\n",
    "\n",
    "The dataset is public and references to its license could be found in the README.md in /data subdirectory of the repo.\n",
    "\n",
    "I took inspiration from https://github.com/yuxiangw/autodp/tree/master/example/private-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autodp\n",
    "#!pip install pyspark\n",
    "#!pip install mxnet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, Trainer\n",
    "from mxnet.gluon.data import DataLoader, ArrayDataset\n",
    "from mxnet.optimizer import Optimizer\n",
    "\n",
    "# shipped with the repo\n",
    "import dpdl_utils\n",
    "\n",
    "# import packages for DP\n",
    "from autodp import rdp_bank, rdp_acct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entry point of our spark app\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"2-class classification\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to download the dataset\n",
    "\n",
    "The dataset we use can downloaded from https://raw.githubusercontent.com/alessio-proietti/dp-sgd-notebook/main/data/bank-additional-full-new-label.csv. \n",
    "\n",
    "It's shipped with the repo itself though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.read_csv(\"data/bank-additional-full-new-label.csv\"))\n",
    "#df.printSchema()\n",
    "#df.show(n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the stages for data processing\n",
    "\n",
    "numericCols = [field for (field, dataType) in df.dtypes if ( dataType != \"string\" )]\n",
    "categoricalCols = [field for (field, dataType) in df.dtypes if (dataType == \"string\" and field != \"y\")]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, outputCols=oheOutputCols)\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexerLabel = StringIndexer(inputCol=\"y\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "labelModel = stringIndexerLabel.fit(df)\n",
    "df = labelModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = df.randomSplit([.75, .25], 24)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler])\n",
    "\n",
    "pipelineModel = pipeline.fit(train)\n",
    "train_df = pipelineModel.transform(train).select(\"label\",\"features\")\n",
    "\n",
    "pipelineModel = pipeline.fit(validation)\n",
    "val_df = pipelineModel.transform(validation).select(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30831, 53) (30831,)\n",
      "(10357, 53) (10357,)\n"
     ]
    }
   ],
   "source": [
    "train_array = nd.array( [row.features.toArray() for row in train_df.collect()])\n",
    "train_label_array = nd.array( [row.label for row in train_df.collect()])\n",
    "\n",
    "val_array = nd.array( [row.features.toArray() for row in val_df.collect()])\n",
    "val_label_array = nd.array( [row.label for row in val_df.collect()])\n",
    "\n",
    "print(train_array.shape, train_label_array.shape)\n",
    "print(val_array.shape, val_label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.random.seed(12345)\n",
    "\n",
    "ctx = mx.cpu()\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArrayDataset(train_array, train_label_array)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = ArrayDataset(val_array, val_label_array)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(units=10, activation='relu'))  # input layer\n",
    "    net.add(nn.Dense(units=10, activation='relu'))   # inner layer 1\n",
    "    net.add(nn.Dense(units=10, activation='relu'))   # inner layer 2\n",
    "    net.add(nn.Dense(units=1))   # output layer: notice, it must have only 1 neuron\n",
    "\n",
    "net.initialize(mx.init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "trainer = Trainer(params=net.collect_params(), optimizer='sgd',\n",
    "                  optimizer_params={'learning_rate': 0.1})\n",
    "accuracy = mx.metric.Accuracy()\n",
    "f1 = mx.metric.F1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mx.optimizer.Optimizer.register\n",
    "class privateSDG(mx.optimizer.Optimizer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = mx.optimizer.Optimizer.create_optimizer('privateSDG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    cumulative_train_loss = 0\n",
    "\n",
    "    for i, (data, label) in enumerate(train_dataloader):\n",
    "        with autograd.record():\n",
    "            # Do forward pass on a batch of training data\n",
    "            output = net(data)\n",
    "\n",
    "            # Calculate loss for the training data batch\n",
    "            loss_result = loss(output, label)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss_result.backward()\n",
    "\n",
    "        # Update parameters of the network\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # sum losses of every batch\n",
    "        cumulative_train_loss += nd.sum(loss_result).asscalar()\n",
    "\n",
    "    return cumulative_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(threshold):\n",
    "    cumulative_val_loss = 0\n",
    "\n",
    "    for i, (val_data, val_ground_truth_class) in enumerate(val_dataloader):\n",
    "        # Do forward pass on a batch of validation data\n",
    "        output = net(val_data)\n",
    "\n",
    "        # Similar to cumulative training loss, calculate cumulative validation loss\n",
    "        cumulative_val_loss += nd.sum(loss(output, val_ground_truth_class)).asscalar()\n",
    "\n",
    "        # getting prediction as a sigmoid\n",
    "        prediction = net(val_data).sigmoid()\n",
    "\n",
    "        # Converting neuron outputs to classes\n",
    "        predicted_classes = mx.nd.ceil(prediction - threshold)\n",
    "\n",
    "        # Update validation accuracy\n",
    "        accuracy.update(val_ground_truth_class, predicted_classes.reshape(-1))\n",
    "\n",
    "        # calculate probabilities of belonging to different classes. F1 metric works only with this notation\n",
    "        prediction = prediction.reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - prediction, prediction, axis=1)\n",
    "\n",
    "        f1.update(val_ground_truth_class, probabilities)\n",
    "\n",
    "    return cumulative_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 1, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 2, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 3, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 4, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 5, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 6, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 7, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 8, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n",
      "Epoch: 9, Training loss: 0.35, Validation loss: 0.35, Validation accuracy: 0.89, F1 score: 0.00\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "threshold = 0.5\n",
    "train_data_size = 30831\n",
    "val_data_size = 10357\n",
    "\n",
    "for e in range(epochs):\n",
    "    avg_train_loss = train_model() / train_data_size\n",
    "    avg_val_loss = validate_model(threshold) / val_data_size\n",
    "\n",
    "    print(\"Epoch: %s, Training loss: %.2f, Validation loss: %.2f, Validation accuracy: %.2f, F1 score: %.2f\" %\n",
    "          (e, avg_train_loss, avg_val_loss, accuracy.get()[1], f1.get()[1]))\n",
    "\n",
    "    # we reset accuracy, so the new epoch's accuracy would be calculated from the blank state\n",
    "    accuracy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
